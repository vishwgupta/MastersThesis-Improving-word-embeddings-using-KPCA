\chapter{Introduction}
\label{cha:intro}

% Important: you have to switch to arabic numbering here!
\pagenumbering{arabic}
\section{Motivation}
According to Prof. Andrew NG, artificial intelligence is like \textit{"the new electricity"}\footnote{https://www.gsb.stanford.edu/insights/andrew-ng-why-ai-new-electricity}. It has revolutionized every field in which it has found its use in. It has also enabled humans to be more efficient by managing their day to day tasks. Examples of such tasks include issuing commands to robots, comprehending the full text of newspaper articles or poetry passages, classifying text for the automatic analysis of emails and their routing to a suitable department. There have been many efforts invested to construct such an intelligent system.\\
A vital part of such a system is that it should easily understand human languages. \textit{"Natural language processing (NLP)"}, a field of AI, which deals with these problems. It can also be seen as a way through which computers can analyze, understand, and derive meaning from the human language. When computers understand our languages, they can organize and structure knowledge to perform tasks such as automatic summarization, translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation etc. very efficiently.\\

\section{Challenges}
The NLP problems are not trivial, making them \textit{"AI-hard problem"} in the field of computer science. This can be attributed to the fact that human languages are not precise. In order to understand a sentence, one should not only understand the words used in it but also the context in which those words are used since a word can have very different meaning when used in different contexts. This makes language understanding ambiguous.\\
The solution comes by focusing on the "atomic units" of a language, \textbf{"words"}. To get the precise results, there is a dire need of a numeric vector representation for the words. These word representations help in extracting relations between various human languages, text or speech data. \\
One way to represent words is in their vectors representation. There are several advantages of using vector representations of the words which include application of the nearest neighbors algorithms on these vectors. When applied on the word, we can obtain clusters of similar meaning words. This can be very useful in text completion, next word prediction and in so many other applications.	

In order to build such a representation, a very basic idea was to use \texttt{one-hot vector}. One hot vector is a very sparse vector which has "1" at the index position of the word in the dictionary and "0" at all other positions. The main limitation of this vector representation is that it encodes no information about the word except its position in the given vocabulary. Such a vector representation depends on the vocabulary size of the text thus excluding the representations of \texttt{out-of-vocabulary words}.\\
To eliminate the first limitation, word embeddings which depends on the context of the text are being proposed. These embeddings are dense, low-dimensional and more informative than the one-hot vector encoding. To obtain such embeddings we need to train a neural network on a very huge amount of data. Such embeddings make use of semantic information of text such that words used in same context cluster together. What these models does not make use of, is the \texttt{sub-word information}. This does not effect so heavily when the language is non-morphological. But for the morphological languages like German and Turkish, we can do better as morphological similarity also embed semantic similarities between words and vice-versa. There have been some recent works such as \texttt{FastText}, which works on this idea of including sub-words in the vocabulary. The model is then trained on this increased vocabulary, learning vector representations of sub-words also. Although this approach eliminates both limitations of \texttt{one-hot vector}, there is significant increase in the vocabulary size which leads to the increased training time. This model still needs to be trained on a huge amount of data to generate high quality vectors.
\section{Thesis Outline}
In this work, we propose an alternative approach which not only make use of the sub-word information but also performs well on smaller datasets. This approach can be applied on both morphological and non-morphological languages. The main challenges we are trying to solve for generating word embeddings are:
\begin{enumerate}
	\item The need to train on a huge text corpus to extract relationship between words.
	\item Word embeddings for out-of-vocabulary words using \textit{n}-grams with significant increase in vocabulary size.
	\item Slow rate of training even after application of optimization techniques such as softmax and negative sampling.
\end{enumerate}
In Chapter 2, we provide all the theoretical background to understand the terminology and techniques used throughout this work. Following is the Chapter 3, which gives a quick overview of the related works dealing with generating word embeddings. Chapter 4 introduces and discusses the approach we proposed to meet the challenges mentioned above. Here we discuss in detail the steps from cleaning the data to obtaining kernel PCA embeddings and finally training a skip gram model for generating word embeddings using the KPCA embeddings. In Chapter 5, we compare our model, KPCA skip gram model with word2vec skip gram model trained using same parameters and finally conclude with Chapter 6 talking about the future work which involves further improvement in the proposed KPCA skip gram model and usage of embeddings in small datasets. 

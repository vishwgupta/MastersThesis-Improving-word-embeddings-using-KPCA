\thispagestyle{empty}

\centerline{\Large{\textbf{Abstract}}}

\vspace{2cm}
Word embeddings have always been an active and crucial research area in the field of Natural Language Processing. This is because of the fact that most of the NLP algorithms use word embeddings at their atomic level. Since this constitutes the backbone of most of the NLP algorithms, researchers, continuously explore ways to make the models which generate these embeddings, more efficient. Some popular models used for generating these embeddings includes distributive skip gram model and CBOW model. These models learn a distributed vector representations that capture a large number of precise syntactic and semantic word relationships. Both of these models, when learning the vector representation of the words, take only the context of the word into account. The sub-structure of the words, which plays a very important role in morphologically rich languages are not taken into account. A more recent model, known as fastText has used an approach to generate word embeddings based on n-grams of the words. This approach learns the vector representation of the word as well as the n-grams in the word, hence enriching the word embedding with sub word vectors.
In this thesis work, we have also tried to follow the approach of enriching the word embeddings, generated using the skip gram model, with the morphological information. This morphological information is derived from computing the KPCA of the words in the vocabulary. One benefit of using the KPCA embeddings of the vocabulary is that it can very easily be extended for the out-of-vocabulary words. Once we have the KPCA embeddings of the words, we can very easily use them as input vectors in the skip gram model. This can be seen as explicitly feeding the network with morphological similarities of the words and letting the network learn semantic and syntactic similarities. We can also see this approach as a better initialization of the model. When we evaluate our approach on the word similarity and analogy tasks, we found that our approach not only achieved better accuracies than the basic skip gram model but also fasten the process of training. We also explored the idea of training a skip gram model fed with KPCA embeddings, on a relatively smaller dataset. In contrast to the basic skip gram model, which needs to be trained on a large dataset to generate meaningful word embeddings, our model can be trained on a smaller dataset and yet produces good quality word embeddings. This approach not only works quite well for a morphologically rich language but also works fairly well for the non-morphological languages. 
\newpage
\thispagestyle{empty}
\rule{0cm}{5cm}
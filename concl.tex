\chapter{Conclusion and Future Work}
\label{cha:concl}

\section{Improvement To the Skip Gram Model}

In this thesis work, we have explored the importance of morphological similarities in generating word embeddings. We took a step by step approach from defining a similarity function between words to generating the kernelized PCA embeddings to feeding the skip gram model with these KPCA embeddings. We also evaluated at each step, how each algorithm performed and which parameters are best suited. The main goal of this work was to explore, the idea of enriching the word embeddings, generated by skip gram model, with the morphological information. This morphological information is encoded in the KPCA embeddings which are obtained by applying KPCA on some defined similarity function of the words. Therefore when we initialized the skip gram model by these morphological similarity words, we found that our model achieve fairly good accuracies in fewer iterations. We have also evaluated the embeddings by intrinsic methods of "word similarity" and "word analogy". In every task, our model has performed better than the basic skip gram model. From the evaluation results, we understood that, a skip gram model can learn similarity between words based on context only when it has seen sufficient number of examples. This can be achieved by either training the skip gram model with a huge dataset or training it with a relatively smaller dataset but with numerous epochs. 
But when we fed the skip gram model with morphological embeddings, it already has a good initial point from where it can improve these embeddings. This works, especially well, when we are training on a morphologically rich language since semantic and morphological similarities converges in these languages therefore feeding the network with KPCA embeddings, gives a boost to the network enabling it to learn meaningful word vectors even faster than the non morphological languages.\\
We also observed that it is possible to obtain fair quality word vectors using KPCA skip gram model in just 1 epoch after training even on a smaller data set. Since skip gram model does not perform well when trained on a relatively smaller data set, our approach can be used in situations where we don't have a huge data set. One such case arises when we have a very new data set such as NEWS articles. Because the current news events changes every often, it is difficult to get good word embeddings for new words which get associated with some context. For example: When Donald Trump won the presidential elections, the newly trained word embeddings for the word 'president' were still lie closer to 'obama' and not close to 'trump'. This is because the skip gram model has not seen enough examples where word 'president' is used in context of 'trump'. In such situations, our approach can do better as we can train on smaller datasets. \\


\section{Improving KPCA Skip Gram Model}
While the KPCA skip gram model, gave good results, there still exists a lot of scope for the improvements which are enumerated as follows:
\begin{itemize}
	\item Selecting number of grams to be considered for generating \textit{n}-gram.
	\begin{itemize}
		\item In this work, we have selected the value of \textit{n} to be 3 and 4 for English and German language for generating \textit{n}-grams. This is an important hyperparmeter which determines the quality of the KPCA embeddings. In this work, we hard coded this parameter but this need to be generalized as n=3, might not give good quality n-grams(morphemes) for some language. This can be generalized by generating different set of similarity measures using different values of \texttt{n = 2,3,4,5,..} and then averaging the similarity for the two words for these set. Thus we can have a generalized model for getting similarity between the words.  
	\end{itemize}
	\item Iterative Kernel PCA for a very large vocabulary
	\begin{itemize}
		\item Since Kernel PCA for matrices of very large dimensions are very computationally expensive, there have been some research going on how to do this iteratively. This is an open research topic that can be explored in future. There are "gain adaptation methods" that improve convergence of the kernel Hebbian algorithm (KHA) for iterative kernel PCA. These methods have demonstrated the scalability by performing kernel PCA on the MNIST database.
	\end{itemize} 
	\item Exploring KPCA fastText
		\begin{itemize}
		\item Although we can obtain the KPCA vector of an out-of-vocabulary word using KPCA projection matrix, but these vector are only enriched with the morphological similarities. We still miss the vector representations encoded with semantic and syntactic similarities. This can be achieved using fastText approach. If we can adapt our KPCA skip gram model according to the fastText, then we can also generate out-of-vocabulary words which are enriched in semantic, syntactic and morphological similarities.  
	\end{itemize} 
\end{itemize}
